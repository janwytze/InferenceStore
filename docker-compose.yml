version: "3.8"
services:
  inference_store:
    build: .
    ports:
      - "50051:50051"
  triton:
    image: nvcr.io/nvidia/tritonserver:23.02-py3
    ports:
      - "8001:8001"
    volumes:
      - "./models:/models"
    command: [ "tritonserver", "--model-repository=/models" ]
